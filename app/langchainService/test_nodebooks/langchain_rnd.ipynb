{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring all runnable functionalities langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import(\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,\n",
    "    RunnablePassthrough\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from functools import partial\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnable pass through maps the previous values to the next as it is usually used for branching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:hello\n",
      "Check: hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass through examples\n",
    "skip = RunnableLambda(lambda x: x)\n",
    "skip_alt  = RunnablePassthrough()\n",
    "# Runnable lambda to convert functions into chainnable components\n",
    "def add_preface(x, preface = \"\"):\n",
    "    if type(x) == str:\n",
    "        print(f\"{preface}{x}\")\n",
    "    else:\n",
    "        print(x)\n",
    "    return x\n",
    "preface_link= RunnableLambda(partial(add_preface, preface=\"1:\"))\n",
    "\n",
    "def Rpreface(preface=\"\"):\n",
    "    return RunnableLambda(partial(add_preface, preface=preface))\n",
    "\n",
    "chain = skip | preface_link | Rpreface(\"Check: \")\n",
    "chain.invoke(\"hello\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates as  strings initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='system, this doesnt works'), HumanMessage(content='hi, this works')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"{test} this doesnt works\"),\n",
    "        (\"user\", \"{input}, this works\")\n",
    "    ]\n",
    ")\n",
    "chain2 = prompt\n",
    "chain2.invoke({\"test\": \"system,\", \"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-component chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> input\n",
      "step 1: testing basic multi chain\n",
      "step 2: testing\n",
      "step 3: TESTING\n",
      "<class 'str'> input\n",
      "step 1: testing basic multi chain\n",
      "step 2: testing\n",
      "step 3: TESTING\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TESTING'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def creat_dict(v, key):\n",
    "    print(type(v), key)\n",
    "    if isinstance(v, dict):\n",
    "        return v\n",
    "    return {key: v}\n",
    "\n",
    "def RInput(key=\"input\"):\n",
    "    return RunnableLambda(partial(creat_dict, key=key))\n",
    "\n",
    "def ROutput(key=\"output\"):\n",
    "    return RunnableLambda(partial(creat_dict, key=key))\n",
    "\n",
    "multi_link_chain1 = (\n",
    "    # passing input as a word or dict, word converts to dict\n",
    "    RInput()  \n",
    "    | skip_alt # pass through\n",
    "    | itemgetter(\"input\") # extracting string, from dict, with key input  \n",
    "    | Rpreface(\"step 1: \") # printing with prefix step\n",
    "    # extracting each word\n",
    "    | {\n",
    "        \"word1\": (lambda x: x.split()[0]),\n",
    "        \"word2\": (lambda x: x.split()[1]),\n",
    "        \"word3\": (lambda x: x.split()[2]),\n",
    "        \"complete\": (lambda x:x)     \n",
    "    }\n",
    "    #extracting word by key\n",
    "    | itemgetter(\"word1\")\n",
    "    | Rpreface(\"step 2: \")\n",
    "    # capitalizing the word\n",
    "    | (lambda x: x.upper())\n",
    "    | Rpreface(\"step 3: \")         \n",
    ")\n",
    "multi_link_chain1.invoke({\"input\": \"testing basic multi chain\"})\n",
    "multi_link_chain1.invoke(\"testing basic multi chain\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Hello World'}\n",
      "<class 'dict'> input\n",
      "{'input': 'Hello World'}\n",
      "C: Hello World\n",
      "{'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
      "E: Hello\n",
      "F: HELLO\n",
      "<class 'str'> output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': 'HELLO'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_link_chain2 = (\n",
    "    Rpreface(\"A: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | RInput()\n",
    "    | Rpreface(\"B: \")\n",
    "    ## Pull-values-from-dictionary utility\n",
    "    | itemgetter(\"input\")\n",
    "    | Rpreface(\"C: \")\n",
    "    ## Anything-in Dictionary-out implicit map\n",
    "    | {\n",
    "        'word1' : (lambda x : x.split()[0]),\n",
    "        'word2' : (lambda x : x.split()[1]),\n",
    "        'words' : (lambda x: x),  ## <- == to RunnablePassthrough()\n",
    "    }\n",
    "    | Rpreface(\"D: \")\n",
    "    | itemgetter(\"word1\")\n",
    "    | Rpreface(\"E: \")\n",
    "    ## Anything-in anything-out lambda application\n",
    "    | RunnableLambda(lambda x: x.upper())\n",
    "    | Rpreface(\"F: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | ROutput()\n",
    ")\n",
    "\n",
    "multi_link_chain2.invoke({\"input\" : \"Hello World\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt chain for decision making. This is just an example for using chains. Its not practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>You are a skilled database assistant for a database containing monitoring data for News Media. Your primary responsibilities include:\n",
      "* **Understanding User Queries:** Process user questions related to the database for News Media, focusing on intent and key information.\n",
      "* **Generating SQL Queries:** If required, accurately translate user questions into efficient and well-structured SQL queries.\n",
      "* **Interpreting Results:** If required, Analyze the query output and provide clear, concise summaries of the findings.\n",
      "</SYS>> \n",
      "Pick the most likely next step based on Users question. If question isnt in you domain of expertise mark it as irrelevant.\n",
      "Choose one option of the following: {options}. Only one word-answers\n",
      "[/INST]\n",
      "ai-llama3-8b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'irrelevant'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prompts import (\n",
    "    CHOICE_PROMPT,\n",
    "    SYSTEM_PROMPT,\n",
    ")\n",
    "\n",
    "print(CHOICE_PROMPT)\n",
    "model = \"ai-llama3-8b\"\n",
    "#model = \"mixtral_8x7b\"\n",
    "api_key = \"nvapi-SN\"\n",
    "print(model)\n",
    "chat_llm = ChatNVIDIA(\n",
    "    model=model,\n",
    "    temperature=0.5,\n",
    "    beam_width=5,\n",
    "    tokens=1024,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    streaming=True,\n",
    "    api_key=api_key\n",
    ")\n",
    "input_msg = \"give me bryani recipie\"\n",
    "options = [\"respond\", \"data\", \"irrelevant\"]\n",
    "zsc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # *chat_history,\n",
    "        (\"system\", CHOICE_PROMPT.format(options=options)),\n",
    "        (\"user\", \"[Options : {options}] {input} = \")\n",
    "    ]\n",
    ")\n",
    "zsc_chain = zsc_prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "selected_option = zsc_chain.invoke({\"input\" : input_msg, \"options\" : options})\n",
    "\n",
    "selected_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'give me bryani recipie', 'options': ['respond', 'data', 'irrelevant']}\n",
      "{'input': 'give me bryani recipie', 'topic': 'irrelevant', 'generation': \"I'm happy to help with your query! However, I need to politely decline as I'm a database assistant for news media monitoring data, and my responsibilities don't include providing recipes. I'm here to help with questions related to the database, such as analyzing news trends, tracking keywords, or providing insights on media coverage. If you have any questions within my scope, I'd be happy to assist you.\"}\n"
     ]
    }
   ],
   "source": [
    "from prompts import (\n",
    "    CHOICE_PROMPT,\n",
    "    SYSTEM_PROMPT,\n",
    ")\n",
    "gen_prompt= ChatPromptTemplate.from_template(SYSTEM_PROMPT)\n",
    "gen_chain = gen_prompt | chat_llm | StrOutputParser()\n",
    "big_chain = (\n",
    "    Rpreface(\"State: \")\n",
    "    ## Manual mapping. Can be useful sometimes and inside branch chains\n",
    "    | {'input' : lambda d: d.get('input'), 'topic' : zsc_chain}\n",
    "    | RunnableAssign({'generation' : gen_chain})\n",
    "    | Rpreface(\"State: \")\n",
    ").invoke({\"input\" : \"give me bryani recipie\", \"options\": options})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running State Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeBase(dimension='general', time_period='general', keywords=[], user_queries=[], action_items=[]) \n",
      "\n",
      "\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"dimension\": {\"title\": \"Dimension\", \"description\": \"items the user is looking for.\", \"default\": \"general\", \"type\": \"string\"}, \"time_period\": {\"title\": \"Time Period\", \"description\": \"Time range specified in which the user is searching.\", \"default\": \"general\", \"type\": \"string\"}, \"keywords\": {\"title\": \"Keywords\", \"description\": \"list of key words that could help with what user is looking for.\", \"default\": [], \"type\": \"array\", \"items\": {}}, \"user_queries\": {\"title\": \"User Queries\", \"description\": \"Unresolved user queries\", \"default\": [], \"type\": \"array\", \"items\": {}}, \"action_items\": {\"title\": \"Action Items\", \"description\": \"Actionable items identified during the conversation.\", \"default\": [], \"type\": \"array\", \"items\": {}}}}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, Union, Optional\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    ## Fields of the BaseModel, which will be validated/assigned when the knowledge base is constructed\n",
    "    dimension: str = Field('general', description=\"items the user is looking for.\")\n",
    "    time_period: str = Field('general', description=\"Time range specified in which the user is searching.\")\n",
    "    keywords: list = Field([], description=\"list of key words that could help with what user is looking for.\")\n",
    "    user_queries: list = Field([], description=\"Unresolved user queries\")\n",
    "    action_items: list = Field([], description=\"Actionable items identified during the conversation.\")\n",
    "\n",
    "print(repr(KnowledgeBase()), \"\\n\\n\")\n",
    "\n",
    "instruct_string = PydanticOutputParser(pydantic_object=KnowledgeBase).get_format_instructions()\n",
    "print(instruct_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'know_base': 'Here is the updated knowledge base:  {\"properties\": {\"dimension\": {\"title\": \"Dimension\", \"description\": \"items the user is looking for.\", \"default\": \"general\", \"type\": \"string\"}, \"time_period\": {\"title\": \"Time Period\", \"description\": \"Time range specified in which the user is searching.\", \"default\": \"general\", \"type\": \"string\"}, \"keywords\": {\"title\": \"Keywords\", \"description\": \"list of key words that could help with what user is looking for.\", \"default\": [], \"type\": \"array\", \"items\": {}}, \"user_queries\": {\"title\": \"User Queries\", \"description\": \"Unresolved user queries\", \"default\": [], \"type\": \"array\", \"items\": {}}, \"action_items\": {\"title\": \"Action Items\", \"description\": \"Actionable items identified during the conversation.\", \"default\": [], \"type\": \"array\", \"items\": {}}} { \"dimension\": \"general\", \"time_period\": \"general\", \"keywords\": [], \"user_queries\": [\"Hi\"], \"action_items\": [] }',\n",
       " 'input': 'Hi'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"]\", \"]\")\n",
    "            .replace(\"[\", \"[\")\n",
    "        )\n",
    "       # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm| StrOutputParser() | preparse\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"The user just responsed. Please update the knowledge base based on the response.\"\n",
    "        \" This information will be acted on to respond to the user in the next interaction.\"\n",
    "        \" Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
    "        \" Do not include anything other the the JSON.\"\n",
    "        \" Update the entries frequently to adapt to the conversation flow.\"\n",
    "        \"\\n{format_instructions}\"\n",
    "    )), (\"user\", \"CURRENT KNOWLEDGE BASE: {know_base}\\nUser: {input}\"),\n",
    "])\n",
    "\n",
    "extractor = RExtract(KnowledgeBase, chat_llm, parser_prompt)\n",
    "info_update = RunnableAssign({'know_base' : extractor})\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "state['input'] = \"Hi\"\n",
    "state = info_update.invoke(state)\n",
    "state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
